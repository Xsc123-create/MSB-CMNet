{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5a23f8-5428-4947-8cf5-47b774956eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Epoch 0/100, Train Loss: 0.2430, Validation_loss: 0.2271,Validation AUC: 0.6922\n",
      "Epoch 1/100, Train Loss: 0.2258, Validation_loss: 0.2213,Validation AUC: 0.7242\n",
      "Epoch 2/100, Train Loss: 0.2212, Validation_loss: 0.2206,Validation AUC: 0.7317\n",
      "Epoch 3/100, Train Loss: 0.2187, Validation_loss: 0.2200,Validation AUC: 0.7405\n",
      "Epoch 4/100, Train Loss: 0.2170, Validation_loss: 0.2164,Validation AUC: 0.7455\n",
      "Epoch 5/100, Train Loss: 0.2147, Validation_loss: 0.2147,Validation AUC: 0.7504\n",
      "Epoch 6/100, Train Loss: 0.2137, Validation_loss: 0.2141,Validation AUC: 0.7520\n",
      "Epoch 7/100, Train Loss: 0.2126, Validation_loss: 0.2132,Validation AUC: 0.7566\n",
      "Epoch 8/100, Train Loss: 0.2113, Validation_loss: 0.2132,Validation AUC: 0.7582\n",
      "Epoch 9/100, Train Loss: 0.2103, Validation_loss: 0.2124,Validation AUC: 0.7624\n",
      "Epoch 10/100, Train Loss: 0.2095, Validation_loss: 0.2115,Validation AUC: 0.7615\n",
      "Epoch 11/100, Train Loss: 0.2086, Validation_loss: 0.2116,Validation AUC: 0.7630\n",
      "Epoch 12/100, Train Loss: 0.2077, Validation_loss: 0.2105,Validation AUC: 0.7662\n",
      "Epoch 13/100, Train Loss: 0.2074, Validation_loss: 0.2107,Validation AUC: 0.7676\n",
      "Epoch 14/100, Train Loss: 0.2066, Validation_loss: 0.2108,Validation AUC: 0.7670\n",
      "Epoch 15/100, Train Loss: 0.2057, Validation_loss: 0.2100,Validation AUC: 0.7682\n",
      "Epoch 16/100, Train Loss: 0.2051, Validation_loss: 0.2098,Validation AUC: 0.7710\n",
      "Epoch 17/100, Train Loss: 0.2043, Validation_loss: 0.2088,Validation AUC: 0.7720\n",
      "Epoch 18/100, Train Loss: 0.2037, Validation_loss: 0.2084,Validation AUC: 0.7748\n",
      "Epoch 19/100, Train Loss: 0.2031, Validation_loss: 0.2084,Validation AUC: 0.7746\n",
      "Epoch 20/100, Train Loss: 0.2026, Validation_loss: 0.2093,Validation AUC: 0.7748\n",
      "Epoch 21/100, Train Loss: 0.2019, Validation_loss: 0.2101,Validation AUC: 0.7750\n",
      "Epoch 22/100, Train Loss: 0.2016, Validation_loss: 0.2079,Validation AUC: 0.7768\n",
      "Epoch 23/100, Train Loss: 0.2010, Validation_loss: 0.2078,Validation AUC: 0.7764\n",
      "Epoch 24/100, Train Loss: 0.2005, Validation_loss: 0.2074,Validation AUC: 0.7779\n",
      "Epoch 25/100, Train Loss: 0.1997, Validation_loss: 0.2067,Validation AUC: 0.7796\n",
      "Epoch 26/100, Train Loss: 0.1994, Validation_loss: 0.2065,Validation AUC: 0.7799\n",
      "Epoch 27/100, Train Loss: 0.1991, Validation_loss: 0.2078,Validation AUC: 0.7801\n",
      "Epoch 28/100, Train Loss: 0.1986, Validation_loss: 0.2072,Validation AUC: 0.7816\n",
      "Epoch 29/100, Train Loss: 0.1978, Validation_loss: 0.2064,Validation AUC: 0.7817\n",
      "Epoch 30/100, Train Loss: 0.1976, Validation_loss: 0.2068,Validation AUC: 0.7819\n",
      "Epoch 31/100, Train Loss: 0.1971, Validation_loss: 0.2067,Validation AUC: 0.7826\n",
      "Epoch 32/100, Train Loss: 0.1965, Validation_loss: 0.2079,Validation AUC: 0.7819\n",
      "Epoch 33/100, Train Loss: 0.1963, Validation_loss: 0.2067,Validation AUC: 0.7822\n",
      "Epoch 34/100, Train Loss: 0.1959, Validation_loss: 0.2057,Validation AUC: 0.7848\n",
      "Epoch 35/100, Train Loss: 0.1956, Validation_loss: 0.2062,Validation AUC: 0.7833\n",
      "Epoch 36/100, Train Loss: 0.1952, Validation_loss: 0.2061,Validation AUC: 0.7839\n",
      "Epoch 37/100, Train Loss: 0.1946, Validation_loss: 0.2049,Validation AUC: 0.7853\n",
      "Epoch 38/100, Train Loss: 0.1945, Validation_loss: 0.2052,Validation AUC: 0.7849\n",
      "Epoch 39/100, Train Loss: 0.1938, Validation_loss: 0.2053,Validation AUC: 0.7870\n",
      "Epoch 40/100, Train Loss: 0.1935, Validation_loss: 0.2051,Validation AUC: 0.7865\n",
      "Epoch 41/100, Train Loss: 0.1933, Validation_loss: 0.2053,Validation AUC: 0.7869\n",
      "Epoch 42/100, Train Loss: 0.1925, Validation_loss: 0.2054,Validation AUC: 0.7856\n",
      "Epoch 43/100, Train Loss: 0.1925, Validation_loss: 0.2053,Validation AUC: 0.7850\n",
      "Epoch 44/100, Train Loss: 0.1920, Validation_loss: 0.2042,Validation AUC: 0.7872\n",
      "Epoch 45/100, Train Loss: 0.1919, Validation_loss: 0.2043,Validation AUC: 0.7883\n",
      "Epoch 46/100, Train Loss: 0.1918, Validation_loss: 0.2046,Validation AUC: 0.7862\n",
      "Epoch 47/100, Train Loss: 0.1909, Validation_loss: 0.2057,Validation AUC: 0.7868\n",
      "Epoch 48/100, Train Loss: 0.1907, Validation_loss: 0.2043,Validation AUC: 0.7881\n",
      "Epoch 49/100, Train Loss: 0.1906, Validation_loss: 0.2046,Validation AUC: 0.7885\n",
      "Epoch 50/100, Train Loss: 0.1905, Validation_loss: 0.2048,Validation AUC: 0.7865\n",
      "Epoch 51/100, Train Loss: 0.1900, Validation_loss: 0.2042,Validation AUC: 0.7864\n",
      "Epoch 52/100, Train Loss: 0.1896, Validation_loss: 0.2048,Validation AUC: 0.7862\n",
      "Epoch 53/100, Train Loss: 0.1894, Validation_loss: 0.2044,Validation AUC: 0.7880\n",
      "Epoch 54/100, Train Loss: 0.1892, Validation_loss: 0.2046,Validation AUC: 0.7886\n",
      "Epoch 55/100, Train Loss: 0.1886, Validation_loss: 0.2045,Validation AUC: 0.7894\n",
      "Epoch 56/100, Train Loss: 0.1885, Validation_loss: 0.2046,Validation AUC: 0.7899\n",
      "Epoch 57/100, Train Loss: 0.1881, Validation_loss: 0.2041,Validation AUC: 0.7880\n",
      "Epoch 58/100, Train Loss: 0.1878, Validation_loss: 0.2046,Validation AUC: 0.7880\n",
      "Epoch 59/100, Train Loss: 0.1879, Validation_loss: 0.2041,Validation AUC: 0.7901\n",
      "Epoch 60/100, Train Loss: 0.1873, Validation_loss: 0.2042,Validation AUC: 0.7898\n",
      "Epoch 61/100, Train Loss: 0.1873, Validation_loss: 0.2040,Validation AUC: 0.7908\n",
      "Epoch 62/100, Train Loss: 0.1869, Validation_loss: 0.2036,Validation AUC: 0.7900\n",
      "Epoch 63/100, Train Loss: 0.1867, Validation_loss: 0.2035,Validation AUC: 0.7894\n",
      "Epoch 64/100, Train Loss: 0.1863, Validation_loss: 0.2040,Validation AUC: 0.7871\n",
      "Epoch 65/100, Train Loss: 0.1864, Validation_loss: 0.2038,Validation AUC: 0.7905\n",
      "Epoch 66/100, Train Loss: 0.1864, Validation_loss: 0.2036,Validation AUC: 0.7903\n",
      "Epoch 67/100, Train Loss: 0.1859, Validation_loss: 0.2052,Validation AUC: 0.7885\n",
      "Epoch 68/100, Train Loss: 0.1858, Validation_loss: 0.2048,Validation AUC: 0.7907\n",
      "Epoch 69/100, Train Loss: 0.1854, Validation_loss: 0.2034,Validation AUC: 0.7905\n",
      "Epoch 70/100, Train Loss: 0.1853, Validation_loss: 0.2045,Validation AUC: 0.7872\n",
      "Epoch 71/100, Train Loss: 0.1852, Validation_loss: 0.2043,Validation AUC: 0.7896\n",
      "Epoch 72/100, Train Loss: 0.1847, Validation_loss: 0.2027,Validation AUC: 0.7920\n",
      "Epoch 73/100, Train Loss: 0.1847, Validation_loss: 0.2034,Validation AUC: 0.7904\n",
      "Epoch 74/100, Train Loss: 0.1843, Validation_loss: 0.2030,Validation AUC: 0.7904\n",
      "Epoch 75/100, Train Loss: 0.1845, Validation_loss: 0.2039,Validation AUC: 0.7902\n",
      "Epoch 76/100, Train Loss: 0.1837, Validation_loss: 0.2031,Validation AUC: 0.7914\n",
      "Epoch 77/100, Train Loss: 0.1837, Validation_loss: 0.2043,Validation AUC: 0.7904\n",
      "Epoch 78/100, Train Loss: 0.1839, Validation_loss: 0.2049,Validation AUC: 0.7897\n",
      "Epoch 79/100, Train Loss: 0.1830, Validation_loss: 0.2041,Validation AUC: 0.7915\n",
      "Epoch 80/100, Train Loss: 0.1833, Validation_loss: 0.2036,Validation AUC: 0.7917\n",
      "Epoch 81/100, Train Loss: 0.1827, Validation_loss: 0.2036,Validation AUC: 0.7912\n",
      "Epoch 82/100, Train Loss: 0.1828, Validation_loss: 0.2054,Validation AUC: 0.7922\n",
      "Epoch 83/100, Train Loss: 0.1833, Validation_loss: 0.2030,Validation AUC: 0.7904\n",
      "Epoch 84/100, Train Loss: 0.1828, Validation_loss: 0.2039,Validation AUC: 0.7904\n",
      "Epoch 85/100, Train Loss: 0.1825, Validation_loss: 0.2047,Validation AUC: 0.7917\n",
      "Epoch 86/100, Train Loss: 0.1823, Validation_loss: 0.2037,Validation AUC: 0.7895\n",
      "Epoch 87/100, Train Loss: 0.1823, Validation_loss: 0.2049,Validation AUC: 0.7905\n",
      "Epoch 88/100, Train Loss: 0.1819, Validation_loss: 0.2055,Validation AUC: 0.7898\n",
      "Epoch 89/100, Train Loss: 0.1826, Validation_loss: 0.2037,Validation AUC: 0.7892\n",
      "Epoch 90/100, Train Loss: 0.1820, Validation_loss: 0.2030,Validation AUC: 0.7909\n",
      "Epoch 91/100, Train Loss: 0.1820, Validation_loss: 0.2045,Validation AUC: 0.7936\n",
      "Epoch 92/100, Train Loss: 0.1819, Validation_loss: 0.2046,Validation AUC: 0.7916\n",
      "Epoch 93/100, Train Loss: 0.1820, Validation_loss: 0.2030,Validation AUC: 0.7906\n",
      "Epoch 94/100, Train Loss: 0.1814, Validation_loss: 0.2031,Validation AUC: 0.7932\n",
      "Epoch 95/100, Train Loss: 0.1815, Validation_loss: 0.2041,Validation AUC: 0.7905\n",
      "Epoch 96/100, Train Loss: 0.1815, Validation_loss: 0.2033,Validation AUC: 0.7917\n",
      "Epoch 97/100, Train Loss: 0.1813, Validation_loss: 0.2035,Validation AUC: 0.7912\n",
      "Epoch 98/100, Train Loss: 0.1808, Validation_loss: 0.2033,Validation AUC: 0.7915\n",
      "Epoch 99/100, Train Loss: 0.1811, Validation_loss: 0.2039,Validation AUC: 0.7915\n",
      "Test_loss:0.2018,Final Test AUC: 0.7952\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# %%capture\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate):\n",
    "        super(DNN, self).__init__()\n",
    "        self.inputs_dim = inputs_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.hidden_units = [inputs_dim] + list(self.hidden_units)\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_units[i], self.hidden_units[i+1]) for i in range(len(self.hidden_units)-1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.0001)\n",
    "\n",
    "        # self.bn = nn.ModuleList([\n",
    "        #     nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        # ])\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, X):\n",
    "        inputs = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](inputs)\n",
    "            fc = self.activation(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            inputs = fc\n",
    "        return inputs\n",
    "\n",
    "class CrossNet(nn.Module):\n",
    "    def __init__(self, in_features, layer_num=2, parameterization='vector', seed=2022):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.parameterization = parameterization\n",
    "        if self.parameterization == 'vector':\n",
    "            self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "        elif self.parameterization == 'matrix':\n",
    "            self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "\n",
    "        for i in range(self.kernels.shape[0]):\n",
    "            nn.init.xavier_normal_(self.kernels[i])\n",
    "        for i in range(self.bias.shape[0]):\n",
    "            nn.init.zeros_(self.bias[0])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_0 = inputs.unsqueeze(2)\n",
    "        x_1 = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            if self.parameterization == 'vector':\n",
    "                x1_w = torch.tensordot(x_1, self.kernels[i], dims=([1], [0]))\n",
    "                dot_ = torch.matmul(x_0, x1_w)\n",
    "                x_1 = dot_ + self.bias[i] + x_1\n",
    "            else:\n",
    "                x1_w = torch.tensordot(self.kernels[i], x_1)\n",
    "                dot_ = x1_w + self.bias[i]\n",
    "                x_1 = x_0 * dot_ + x_1\n",
    "        x_1 = torch.squeeze(x_1, dim=2)\n",
    "        return x_1\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns, cross_num=2,\n",
    "                 cross_param='vector', dnn_hidden_units=(128, 128), init_std=0.0001, seed=2022, l2_reg=0.00001,\n",
    "                 drop_rate=0.5):\n",
    "        super(DCN, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dnn_hidden_units = dnn_hidden_units\n",
    "        self.cross_num = 2\n",
    "        self.cross_param = cross_param\n",
    "        self.drop_rate = drop_rate\n",
    "        self.l2_reg = 0.00001\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.dense_feature_columns = list(filter(lambda x:x[1]=='dense', dnn_feature_columns))\n",
    "        self.sparse_feature_columns = list(filter(lambda x:x[1]=='sparse', dnn_feature_columns))\n",
    "\n",
    "        self.embedding_dic = nn.ModuleDict({feat[0]:nn.Embedding(feat_size[feat[0]], self.embedding_size, sparse=False)\n",
    "                                            for feat in self.sparse_feature_columns})\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        start = 0\n",
    "        for feat in self.feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "\n",
    "        inputs_dim = len(self.dense_feature_columns)+self.embedding_size*len(self.sparse_feature_columns)\n",
    "\n",
    "        self.dnn = DNN(inputs_dim,self.dnn_hidden_units, 0.5)\n",
    "\n",
    "        self.crossnet = CrossNet(inputs_dim, layer_num=self.cross_num, parameterization=self.cross_param)\n",
    "        self.dnn_linear = nn.Linear(inputs_dim+dnn_hidden_units[-1], 1, bias=False)\n",
    "\n",
    "        # dnn_hidden_units = [len(feat_size)] + list(dnn_hidden_units) + [1]\n",
    "        # self.linear = nn.ModuleList([\n",
    "        #     nn.Linear(dnn_hidden_units[i], dnn_hidden_units[i+1]) for i in range(len(dnn_hidden_units)-1)\n",
    "        # ])\n",
    "        # for name, tensor in self.linear.named_parameters():\n",
    "        #     if 'weight' in name:\n",
    "        #         nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # logit = X\n",
    "        # for i in range(len(self.linear)):\n",
    "        #     fc = self.linear[i](logit)\n",
    "        #     fc = self.act(fc)\n",
    "        #     fc = self.dropout(fc)\n",
    "        #     logit = fc\n",
    "\n",
    "        sparse_embedding = [self.embedding_dic[feat[0]](X[:, self.feature_index[feat[0]]].long()).reshape(X.shape[0], 1, -1)\n",
    "                            for feat in self.sparse_feature_columns]\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)\n",
    "\n",
    "        # 拉直 本来是 [batch_size, sparase特征数, 嵌入维度] =》 [batch_size, sparase特征数 * 嵌入维度]\n",
    "        sparse_input = torch.flatten(sparse_input, start_dim=1)\n",
    "\n",
    "        dnn_input = torch.cat((dense_input, sparse_input), dim=1)\n",
    "\n",
    "        # print('sparse input size', sparse_input.shape)\n",
    "        # print('dense input size', dense_input.shape)\n",
    "        # print('dnn input size', dnn_input.shape)\n",
    "\n",
    "        deep_out = self.dnn(dnn_input)\n",
    "        cross_out = self.crossnet(dnn_input)\n",
    "        stack_out = torch.cat((cross_out, deep_out), dim=-1)\n",
    "\n",
    "        # logit += self.dnn_linear(stack_out)\n",
    "        final= self.dnn_linear(stack_out)\n",
    "        #print('logit size', logit.shape)\n",
    "        y_pred = torch.sigmoid(final)\n",
    "        #print('y_pred', y_pred.shape)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size =500\n",
    "    lr = 1e-3\n",
    "    wd = 1e-5\n",
    "    epoches = 100\n",
    "    seed = 2022\n",
    "    embedding_size =10\n",
    "    device = 'cuda:0'\n",
    "    # pd.set_option('display.max_rows', None)  # 显示数据中所有的列\n",
    "    data = pd.read_csv('vehicle_data_model_50w_3.csv')\n",
    "    # data = pd.read_csv('vehicle_data_model_30w_2.csv')\n",
    "    dense_feature= ['driver_auth_success_days','cargo_search_cnt_3','cargo_search_cnt_7','scan_cargo_cnt_3','scan_cargo_cnt_7','click_cargo_cnt_3_x','click_cargo_cnt_7','call_cnt_3_driver','call_cnt_7_driver',\n",
    "               'shipper_auth_success_days','exposure_cargo_cnt_3','exposure_cnt_3','click_cargo_cnt_3_y','click_cnt_3','cargo_weight','vector_regular_subscribe_line',\n",
    "                       'vector_regular_cargo_line_all','vector_regular_cargo_truck_type_all','vector_regular_cargo_truck_length_all','vector_regular_cargo_line_30',\n",
    "                       'vector_regular_cargo_truck_type_30','vector_regular_cargo_truck_length_30']\n",
    "\n",
    "\n",
    "    # 假设你的数据集中包含'label'列，并且dense_feature已经定义\n",
    "    sparse_feature  = data.drop(columns=['label'] + dense_feature).columns.tolist()\n",
    "    print(len(sparse_feature))\n",
    "    pd.options.display.max_rows = None  # 显示所有列\n",
    "    data[sparse_feature]=data[sparse_feature].astype('uint8')\n",
    "    target = ['label']\n",
    "\n",
    "    feat_sizes = {}  # 初始化一个空字典 feat_sizes。\n",
    "    feat_sizes_dense = {feat: 1 for feat in dense_feature}#这里将稠密特征的维度大小设置为1，因为这些特征不需要经过 Embedding 层，直接作为输入。\n",
    "    # 对每个稀疏特征创建一个键值对，键为特征名称，值为该特征在数据中唯一取值的数量（即不同的类别个数）。\n",
    "    feat_sizes_sparse = {feat: len(data[feat].unique()) for feat in sparse_feature}\n",
    "    # 将稠密特征和稀疏特征的维度大小更新到 feat_sizes 字典中，得到包含所有特征维度大小信息的字典 feat_sizes。\n",
    "    feat_sizes.update(feat_sizes_dense)\n",
    "    feat_sizes.update(feat_sizes_sparse)\n",
    "    # for feat in sparse_feature:\n",
    "    #     lbe = LabelEncoder()  #使用LabelEncoder类对每个稀疏特征进行编码,将每个特征的字符串类型的值映射成整数编号\n",
    "    #     data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "\n",
    "    # 定义fixlen_feature_columns，包含了所有特征的名称和类型（sparse或dense）。\n",
    "    fixlen_feature_columns = [(feat, 'sparse') for feat in sparse_feature] + [(feat, 'dense') for feat in dense_feature]\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    # train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "     # 数据集划分\n",
    "    train, test = train_test_split(data, test_size=0.3, random_state=seed)\n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # DataLoader准备\n",
    "    def create_data_loader(df, batch_size):\n",
    "        labels = pd.DataFrame(df['label'])\n",
    "        features = df.drop(columns=['label'])\n",
    "        tensor_data = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(labels)))\n",
    "        return DataLoader(tensor_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    train_loader = create_data_loader(train, batch_size)\n",
    "    validation_loader = create_data_loader(validation, batch_size)\n",
    "    test_loader = create_data_loader(test, batch_size)\n",
    "\n",
    "    # 模型初始化\n",
    "    # model = FiBiNET(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns)\n",
    "    device = 'cuda:0'\n",
    "    model = DCN(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns).to(device)\n",
    "    loss_func = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 早停策略参数\n",
    "    early_stopping_threshold = 0.80\n",
    "    best_validation_auc = 0\n",
    "    \n",
    "    # 定义一个函数来计算给定数据加载器上的平均损失和AUC\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device).float(), y.to(device).float()\n",
    "                y_hat = model(x)\n",
    "                loss = loss_func(y_hat, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_samples += x.size(0)\n",
    "        avg_loss = total_loss / total_samples\n",
    "        auc = get_auc(loader, model)\n",
    "        return avg_loss, auc\n",
    "\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        total_loss_epoch = 0.0\n",
    "        total_tmp = 0\n",
    "        model.train()\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            total_tmp += 1\n",
    "\n",
    "        # 验证集评估\n",
    "        validation_loss, validation_auc = evaluate(validation_loader)\n",
    "        print(\n",
    "            f'Epoch {epoch}/{epoches}, Train Loss: {total_loss_epoch / total_tmp:.4f}, Validation_loss: {validation_loss:.4f},Validation AUC: {validation_auc:.4f}')\n",
    "\n",
    "        # 更新最佳验证集AUC\n",
    "        if validation_auc > best_validation_auc:\n",
    "            best_validation_auc = validation_auc\n",
    "\n",
    "        # 早停判断\n",
    "        if validation_auc >= early_stopping_threshold:\n",
    "            print(f'Early stopping triggered at epoch {epoch}, Validation AUC: {validation_auc:.4f}')\n",
    "            break\n",
    "\n",
    "    # 测试集评估\n",
    "    test_loss, final_test_auc = evaluate(test_loader)\n",
    "    print(f'Test_loss:{test_loss:.4f},Final Test AUC: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d6c26-d664-4b22-83ee-d3fbf0550c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404087e5-4aad-44b1-a03d-25a4b88dd5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
