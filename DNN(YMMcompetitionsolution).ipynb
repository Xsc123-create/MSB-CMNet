{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da8155c-2837-4376-87b2-1d9f0cbbdb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Epoch 0/25, Train Loss: 0.2500, Validation_loss: 0.2282,Validation AUC: 0.6797\n",
      "Epoch 1/25, Train Loss: 0.2352, Validation_loss: 0.2252,Validation AUC: 0.7141\n",
      "Epoch 2/25, Train Loss: 0.2289, Validation_loss: 0.2188,Validation AUC: 0.7367\n",
      "Epoch 3/25, Train Loss: 0.2245, Validation_loss: 0.2176,Validation AUC: 0.7443\n",
      "Epoch 4/25, Train Loss: 0.2216, Validation_loss: 0.2178,Validation AUC: 0.7465\n",
      "Epoch 5/25, Train Loss: 0.2195, Validation_loss: 0.2147,Validation AUC: 0.7556\n",
      "Epoch 6/25, Train Loss: 0.2180, Validation_loss: 0.2133,Validation AUC: 0.7583\n",
      "Epoch 7/25, Train Loss: 0.2169, Validation_loss: 0.2134,Validation AUC: 0.7610\n",
      "Epoch 8/25, Train Loss: 0.2154, Validation_loss: 0.2131,Validation AUC: 0.7633\n",
      "Epoch 9/25, Train Loss: 0.2141, Validation_loss: 0.2114,Validation AUC: 0.7674\n",
      "Epoch 10/25, Train Loss: 0.2131, Validation_loss: 0.2102,Validation AUC: 0.7706\n",
      "Epoch 11/25, Train Loss: 0.2121, Validation_loss: 0.2105,Validation AUC: 0.7710\n",
      "Epoch 12/25, Train Loss: 0.2112, Validation_loss: 0.2091,Validation AUC: 0.7741\n",
      "Epoch 13/25, Train Loss: 0.2099, Validation_loss: 0.2083,Validation AUC: 0.7758\n",
      "Epoch 14/25, Train Loss: 0.2087, Validation_loss: 0.2082,Validation AUC: 0.7774\n",
      "Epoch 15/25, Train Loss: 0.2075, Validation_loss: 0.2090,Validation AUC: 0.7778\n",
      "Epoch 16/25, Train Loss: 0.2073, Validation_loss: 0.2070,Validation AUC: 0.7795\n",
      "Epoch 17/25, Train Loss: 0.2064, Validation_loss: 0.2066,Validation AUC: 0.7808\n",
      "Epoch 18/25, Train Loss: 0.2053, Validation_loss: 0.2065,Validation AUC: 0.7821\n",
      "Epoch 19/25, Train Loss: 0.2044, Validation_loss: 0.2055,Validation AUC: 0.7842\n",
      "Epoch 20/25, Train Loss: 0.2038, Validation_loss: 0.2048,Validation AUC: 0.7858\n",
      "Epoch 21/25, Train Loss: 0.2029, Validation_loss: 0.2054,Validation AUC: 0.7860\n",
      "Epoch 22/25, Train Loss: 0.2020, Validation_loss: 0.2051,Validation AUC: 0.7851\n",
      "Epoch 23/25, Train Loss: 0.2013, Validation_loss: 0.2052,Validation AUC: 0.7855\n",
      "Epoch 24/25, Train Loss: 0.2004, Validation_loss: 0.2044,Validation AUC: 0.7877\n",
      "Test_loss:0.2021,Final Test AUC: 0.7921\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# %%capture\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate):\n",
    "        super(DNN, self).__init__()\n",
    "        self.inputs_dim = inputs_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.hidden_units = [inputs_dim] + list(self.hidden_units)\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_units[i], self.hidden_units[i+1]) for i in range(len(self.hidden_units)-1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.0001)\n",
    "\n",
    "        # self.bn = nn.ModuleList([\n",
    "        #     nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        # ])\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, X):\n",
    "        inputs = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](inputs)\n",
    "            fc = self.activation(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            inputs = fc\n",
    "        return inputs\n",
    "\n",
    "# class CrossNet(nn.Module):\n",
    "#     def __init__(self, in_features, layer_num=2, parameterization='vector', seed=2022):\n",
    "#         super(CrossNet, self).__init__()\n",
    "#         self.layer_num = layer_num\n",
    "#         self.parameterization = parameterization\n",
    "#         if self.parameterization == 'vector':\n",
    "#             self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "#         elif self.parameterization == 'matrix':\n",
    "#             self.kernels = nn.Parameter(torch.Tensor(self.layer_num, in_features, in_features))\n",
    "#         self.bias = nn.Parameter(torch.Tensor(self.layer_num, in_features, 1))\n",
    "\n",
    "#         for i in range(self.kernels.shape[0]):\n",
    "#             nn.init.xavier_normal_(self.kernels[i])\n",
    "#         for i in range(self.bias.shape[0]):\n",
    "#             nn.init.zeros_(self.bias[0])\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         x_0 = inputs.unsqueeze(2)\n",
    "#         x_1 = x_0\n",
    "#         for i in range(self.layer_num):\n",
    "#             if self.parameterization == 'vector':\n",
    "#                 x1_w = torch.tensordot(x_1, self.kernels[i], dims=([1], [0]))\n",
    "#                 dot_ = torch.matmul(x_0, x1_w)\n",
    "#                 x_1 = dot_ + self.bias[i] + x_1\n",
    "#             else:\n",
    "#                 x1_w = torch.tensordot(self.kernels[i], x_1)\n",
    "#                 dot_ = x1_w + self.bias[i]\n",
    "#                 x_1 = x_0 * dot_ + x_1\n",
    "#         x_1 = torch.squeeze(x_1, dim=2)\n",
    "#         return x_1\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns, cross_num=2,\n",
    "                 cross_param='vector', dnn_hidden_units=(128, 128), init_std=0.0001, seed=2022, l2_reg=0.00001,\n",
    "                 drop_rate=0.5):\n",
    "        super(DCN, self).__init__()\n",
    "        self.feat_size = feat_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dnn_hidden_units = dnn_hidden_units\n",
    "        self.cross_num = 2\n",
    "        self.cross_param = cross_param\n",
    "        self.drop_rate = drop_rate\n",
    "        self.l2_reg = 0.00001\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.dense_feature_columns = list(filter(lambda x:x[1]=='dense', dnn_feature_columns))\n",
    "        self.sparse_feature_columns = list(filter(lambda x:x[1]=='sparse', dnn_feature_columns))\n",
    "\n",
    "        self.embedding_dic = nn.ModuleDict({feat[0]:nn.Embedding(feat_size[feat[0]], self.embedding_size, sparse=False)\n",
    "                                            for feat in self.sparse_feature_columns})\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        start = 0\n",
    "        for feat in self.feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "\n",
    "        inputs_dim = len(self.dense_feature_columns)+self.embedding_size*len(self.sparse_feature_columns)\n",
    "\n",
    "        self.dnn = DNN(inputs_dim,self.dnn_hidden_units, 0.5)\n",
    "\n",
    "        # self.crossnet = CrossNet(inputs_dim, layer_num=self.cross_num, parameterization=self.cross_param)\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1], 1, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "\n",
    "\n",
    "        sparse_embedding = [self.embedding_dic[feat[0]](X[:, self.feature_index[feat[0]]].long()).reshape(X.shape[0], 1, -1)\n",
    "                            for feat in self.sparse_feature_columns]\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)\n",
    "\n",
    "        # 拉直 本来是 [batch_size, sparase特征数, 嵌入维度] =》 [batch_size, sparase特征数 * 嵌入维度]\n",
    "        sparse_input = torch.flatten(sparse_input, start_dim=1)\n",
    "\n",
    "        dnn_input = torch.cat((dense_input, sparse_input), dim=1)\n",
    "\n",
    "        # print('sparse input size', sparse_input.shape)\n",
    "        # print('dense input size', dense_input.shape)\n",
    "        # print('dnn input size', dnn_input.shape)\n",
    "\n",
    "        deep_out = self.dnn(dnn_input)\n",
    "        # cross_out = self.crossnet(dnn_input)\n",
    "        # stack_out = torch.cat((cross_out, deep_out), dim=-1)\n",
    "\n",
    "        # logit += self.dnn_linear(stack_out)\n",
    "        final= self.dnn_linear(deep_out)\n",
    "        #print('logit size', logit.shape)\n",
    "        y_pred = torch.sigmoid(final)\n",
    "        #print('y_pred', y_pred.shape)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size =500\n",
    "    lr = 1e-3\n",
    "    wd = 1e-5\n",
    "    epoches = 25\n",
    "    seed = 2022\n",
    "    embedding_size =20\n",
    "    device = 'cuda:0'\n",
    "    # pd.set_option('display.max_rows', None)  # 显示数据中所有的列\n",
    "    data = pd.read_csv('vehicle_data_model_50w_3.csv')\n",
    "    # data = pd.read_csv('vehicle_data_model_30w_2.csv')\n",
    "    dense_feature= ['driver_auth_success_days','cargo_search_cnt_3','cargo_search_cnt_7','scan_cargo_cnt_3','scan_cargo_cnt_7','click_cargo_cnt_3_x','click_cargo_cnt_7','call_cnt_3_driver','call_cnt_7_driver',\n",
    "               'shipper_auth_success_days','exposure_cargo_cnt_3','exposure_cnt_3','click_cargo_cnt_3_y','click_cnt_3','cargo_weight','vector_regular_subscribe_line',\n",
    "                       'vector_regular_cargo_line_all','vector_regular_cargo_truck_type_all','vector_regular_cargo_truck_length_all','vector_regular_cargo_line_30',\n",
    "                       'vector_regular_cargo_truck_type_30','vector_regular_cargo_truck_length_30']\n",
    "\n",
    "\n",
    "    # 假设你的数据集中包含'label'列，并且dense_feature已经定义\n",
    "    sparse_feature  = data.drop(columns=['label'] + dense_feature).columns.tolist()\n",
    "    print(len(sparse_feature))\n",
    "    pd.options.display.max_rows = None  # 显示所有列\n",
    "    data[sparse_feature]=data[sparse_feature].astype('uint8')\n",
    "    target = ['label']\n",
    "\n",
    "    feat_sizes = {}  # 初始化一个空字典 feat_sizes。\n",
    "    feat_sizes_dense = {feat: 1 for feat in dense_feature}#这里将稠密特征的维度大小设置为1，因为这些特征不需要经过 Embedding 层，直接作为输入。\n",
    "    # 对每个稀疏特征创建一个键值对，键为特征名称，值为该特征在数据中唯一取值的数量（即不同的类别个数）。\n",
    "    feat_sizes_sparse = {feat: len(data[feat].unique()) for feat in sparse_feature}\n",
    "    # 将稠密特征和稀疏特征的维度大小更新到 feat_sizes 字典中，得到包含所有特征维度大小信息的字典 feat_sizes。\n",
    "    feat_sizes.update(feat_sizes_dense)\n",
    "    feat_sizes.update(feat_sizes_sparse)\n",
    "    # for feat in sparse_feature:\n",
    "    #     lbe = LabelEncoder()  #使用LabelEncoder类对每个稀疏特征进行编码,将每个特征的字符串类型的值映射成整数编号\n",
    "    #     data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "\n",
    "    # 定义fixlen_feature_columns，包含了所有特征的名称和类型（sparse或dense）。\n",
    "    fixlen_feature_columns = [(feat, 'sparse') for feat in sparse_feature] + [(feat, 'dense') for feat in dense_feature]\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    # train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "     # 数据集划分\n",
    "    train, test = train_test_split(data, test_size=0.3, random_state=seed)\n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # DataLoader准备\n",
    "    def create_data_loader(df, batch_size):\n",
    "        labels = pd.DataFrame(df['label'])\n",
    "        features = df.drop(columns=['label'])\n",
    "        tensor_data = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(labels)))\n",
    "        return DataLoader(tensor_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    train_loader = create_data_loader(train, batch_size)\n",
    "    validation_loader = create_data_loader(validation, batch_size)\n",
    "    test_loader = create_data_loader(test, batch_size)\n",
    "\n",
    "    # 模型初始化\n",
    "    # model = FiBiNET(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns)\n",
    "    device = 'cuda:0'\n",
    "    model = DCN(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns).to(device)\n",
    "    loss_func = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 早停策略参数\n",
    "    early_stopping_threshold = 0.80\n",
    "    best_validation_auc = 0\n",
    "    \n",
    "    # 定义一个函数来计算给定数据加载器上的平均损失和AUC\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device).float(), y.to(device).float()\n",
    "                y_hat = model(x)\n",
    "                loss = loss_func(y_hat, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_samples += x.size(0)\n",
    "        avg_loss = total_loss / total_samples\n",
    "        auc = get_auc(loader, model)\n",
    "        return avg_loss, auc\n",
    "\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        total_loss_epoch = 0.0\n",
    "        total_tmp = 0\n",
    "        model.train()\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            total_tmp += 1\n",
    "\n",
    "        # 验证集评估\n",
    "        validation_loss, validation_auc = evaluate(validation_loader)\n",
    "        print(\n",
    "            f'Epoch {epoch}/{epoches}, Train Loss: {total_loss_epoch / total_tmp:.4f}, Validation_loss: {validation_loss:.4f},Validation AUC: {validation_auc:.4f}')\n",
    "\n",
    "        # 更新最佳验证集AUC\n",
    "        if validation_auc > best_validation_auc:\n",
    "            best_validation_auc = validation_auc\n",
    "\n",
    "        # 早停判断\n",
    "        if validation_auc >= early_stopping_threshold:\n",
    "            print(f'Early stopping triggered at epoch {epoch}, Validation AUC: {validation_auc:.4f}')\n",
    "            break\n",
    "\n",
    "    # 测试集评估\n",
    "    test_loss, final_test_auc = evaluate(test_loader)\n",
    "    print(f'Test_loss:{test_loss:.4f},Final Test AUC: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6e598-dec0-4356-845b-82bdfa92fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
