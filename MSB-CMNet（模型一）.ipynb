{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2393b3e6-d9d3-4c4c-a808-64233f88ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据维度: (500000, 40) 标签比例: label\n",
      "0    467495\n",
      "1     32505\n",
      "Name: count, dtype: int64\n",
      "训练集标签汇总:\n",
      " label\n",
      "0    378764\n",
      "1     26236\n",
      "Name: count, dtype: int64\n",
      "验证集标签汇总:\n",
      " label\n",
      "0    42034\n",
      "1     2966\n",
      "Name: count, dtype: int64\n",
      "测试集标签汇总:\n",
      " label\n",
      "0    46697\n",
      "1     3303\n",
      "Name: count, dtype: int64\n",
      "Epoch 0/12, Train_loss: 0.2283, Validation_loss: 0.2174,Validation AUC: 0.7543\n",
      "Epoch 1/12, Train_loss: 0.2136, Validation_loss: 0.2130,Validation AUC: 0.7702\n",
      "Epoch 2/12, Train_loss: 0.2084, Validation_loss: 0.2095,Validation AUC: 0.7810\n",
      "Epoch 3/12, Train_loss: 0.2038, Validation_loss: 0.2063,Validation AUC: 0.7899\n",
      "Epoch 4/12, Train_loss: 0.1999, Validation_loss: 0.2078,Validation AUC: 0.7971\n",
      "Epoch 5/12, Train_loss: 0.1957, Validation_loss: 0.2030,Validation AUC: 0.8000\n",
      "Epoch 6/12, Train_loss: 0.1916, Validation_loss: 0.2019,Validation AUC: 0.8009\n",
      "Epoch 7/12, Train_loss: 0.1876, Validation_loss: 0.2002,Validation AUC: 0.8078\n",
      "Epoch 8/12, Train_loss: 0.1836, Validation_loss: 0.2010,Validation AUC: 0.8106\n",
      "Epoch 9/12, Train_loss: 0.1792, Validation_loss: 0.1982,Validation AUC: 0.8138\n",
      "Epoch 10/12, Train_loss: 0.1757, Validation_loss: 0.1977,Validation AUC: 0.8155\n",
      "Epoch 11/12, Train_loss: 0.1710, Validation_loss: 0.1969,Validation AUC: 0.8167\n",
      "Test_loss:0.1985,Final Test AUC: 0.8135\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "import os\n",
    "\n",
    "def get_metrics(loader, model):\n",
    "    pred, target = [], []  # 初始化预测结果和真实标签的列表\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_pred = model(x)\n",
    "            pred.append(y_pred.cpu().numpy())\n",
    "            target.append(y.cpu().numpy())\n",
    "\n",
    "    target = np.concatenate(target)\n",
    "    pred = np.concatenate(pred)\n",
    "\n",
    "    logloss = log_loss(target, pred)  # 计算 log loss\n",
    "    auc = roc_auc_score(target, pred)  # 计算 AUC\n",
    "    return logloss, auc\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_layers, dropout=0.5, output_layer=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = list()\n",
    "        for i in mlp_layers:\n",
    "            layers.append(torch.nn.Linear(input_dim, i))\n",
    "            layers.append(torch.nn.BatchNorm1d(i))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(p=dropout))\n",
    "            input_dim = i\n",
    "\n",
    "        if output_layer:\n",
    "            layers.append(torch.nn.Linear(input_dim, 1))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "        self._init_weight_()\n",
    "\n",
    "    def _init_weight_(self):\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class SENETLayer(nn.Module):\n",
    "    def __init__(self, filed_size, reduction_ratio):\n",
    "        super(SENETLayer, self).__init__()\n",
    "        self.reduction_size = max(1, filed_size // reduction_ratio)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(filed_size, self.reduction_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.reduction_size, filed_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        Z = torch.mean(inputs, dim=2)\n",
    "        A = self.excitation(Z)\n",
    "        V = torch.mul(inputs, torch.unsqueeze(A, dim=2))\n",
    "        return V\n",
    "\n",
    "\n",
    "class BilinearInteraction(nn.Module):\n",
    "    def __init__(self, filed_size, embedding_size, bilinear_type='each'):\n",
    "        super(BilinearInteraction, self).__init__()\n",
    "        self.bilinear_type = bilinear_type\n",
    "        self.bilinear = nn.ModuleList()\n",
    "        if self.bilinear_type == 'all':\n",
    "            self.bilinear = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "        elif self.bilinear_type == 'each':\n",
    "            for _ in range(filed_size):\n",
    "                self.bilinear.append(nn.Linear(embedding_size, embedding_size, bias=False))\n",
    "        elif self.bilinear_type == 'interaction':\n",
    "            for _, _ in itertools.combinations(range(filed_size), 2):\n",
    "                self.bilinear.append(nn.Linear(embedding_size, embedding_size, bias=False))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.split(inputs, 1, dim=1)\n",
    "        if self.bilinear_type == 'all':\n",
    "            p = [torch.mul(self.bilinear(v_i), v_j) for v_i, v_j in itertools.combinations(inputs, 2)]\n",
    "        elif self.bilinear_type == 'each':\n",
    "            p = [torch.mul(self.bilinear[i](inputs[i]), inputs[j]) for i, j in\n",
    "                 itertools.combinations(range(len(inputs)), 2)]\n",
    "        elif self.bilinear_type == 'interaction':\n",
    "            p = [torch.mul(bilinear(v[0]), v[1]) for v, bilinear in\n",
    "                 zip(itertools.combinations(inputs, 2), self.bilinear)]\n",
    "        return torch.cat(p, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class CrossNet(nn.Module):\n",
    "    def __init__(self, input_dim, layer_num):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.w = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(input_dim, 1, bias=False) for _ in range(layer_num)\n",
    "        ])\n",
    "        self.b = torch.nn.ParameterList([torch.nn.Parameter(\n",
    "            torch.zeros((input_dim,))) for _ in range(layer_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        for i in range(self.layer_num):\n",
    "            xw = self.w[i](x)\n",
    "            x = x0 * xw + self.b[i] + x\n",
    "        return x\n",
    "\n",
    "class MSB_CMNet(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns,\n",
    "                 mlp_layers=(256, 128), num_heads=2,reduction_ratio=1,  cross_num=3, drop_rate=0.5):\n",
    "        super(MSB_CMNet, self).__init__()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.sparse_feature_columns = list(filter(lambda x: x[1] == 'sparse', dnn_feature_columns))\n",
    "        self.embedding_dic = nn.ModuleDict({\n",
    "            feat[0]: nn.Embedding(feat_size[feat[0]], embedding_size, sparse=False) for feat in\n",
    "            self.sparse_feature_columns\n",
    "        })\n",
    "        self.dense_feature_columns = list(filter(lambda x: x[1] == 'dense', dnn_feature_columns))\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        # 为特征名称建立索引位置（整数）的映射关系，并从 0 开始递增，确保不同特征拥有不同的索引位置。\n",
    "        start = 0\n",
    "        for feat in feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "        # 引入多头注意力机制\n",
    "        self.multi_head_attention = nn.MultiheadAttention(embedding_size, num_heads)\n",
    "        self.field_size = len(self.embedding_dic)\n",
    "        self.SE = SENETLayer(self.field_size, reduction_ratio)\n",
    "        self.Bilinear = BilinearInteraction(self.field_size, embedding_size)\n",
    "        dim = self.field_size * (self.field_size - 1) * embedding_size + len(self.dense_feature_columns)\n",
    "        self.mlp = MLP(dim, mlp_layers)\n",
    "        self.crossnet = CrossNet(dim, cross_num)\n",
    "        self.dnn_linear = nn.Linear(dim + mlp_layers[-1], 1, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sparse_embedding = [\n",
    "            self.embedding_dic[feat[0]](X[:, self.feature_index[feat[0]]].long()).reshape(X.shape[0], 1, -1)\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)\n",
    "        multi_head_output, _ = self.multi_head_attention(sparse_input, sparse_input, sparse_input)\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "        senet_output = self.SE(sparse_input)\n",
    "        senet_bilinear_out = self.Bilinear(senet_output)\n",
    "        bilinear_out = self.Bilinear(multi_head_output)\n",
    "        mlp_crossnet_input = torch.flatten(torch.cat((senet_bilinear_out, bilinear_out), dim=1), start_dim=1)\n",
    "        final_mlp_crossnet_input = torch.cat((mlp_crossnet_input, dense_input), dim=1)\n",
    "        dnn_out = self.mlp(final_mlp_crossnet_input)\n",
    "        cross_out = self.crossnet(final_mlp_crossnet_input)\n",
    "        final_out = torch.cat((cross_out, dnn_out), dim=-1)\n",
    "        final_logit = self.dnn_linear(final_out)\n",
    "        y_pred = torch.sigmoid(final_logit)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size = 1024\n",
    "    lr = 1e-3\n",
    "    wd = 1e-5\n",
    "    epoches = 12\n",
    "    seed = 2022\n",
    "    embedding_size = 20\n",
    "    #device = 'cuda:0'\n",
    "    device = 'cpu'\n",
    "    # pd.set_option('display.max_rows', None)\n",
    "    data = pd.read_csv('vehicle_data_model_50w_3.csv')\n",
    "    dense_feature = ['driver_auth_success_days', 'cargo_search_cnt_3', 'cargo_search_cnt_7', 'scan_cargo_cnt_3',\n",
    "                     'scan_cargo_cnt_7', 'click_cargo_cnt_3_x', 'click_cargo_cnt_7', 'call_cnt_3_driver',\n",
    "                     'call_cnt_7_driver',\n",
    "                     'shipper_auth_success_days', 'exposure_cargo_cnt_3', 'exposure_cnt_3', 'click_cargo_cnt_3_y',\n",
    "                     'click_cnt_3', 'cargo_weight', 'vector_regular_subscribe_line',\n",
    "                     'vector_regular_cargo_line_all', 'vector_regular_cargo_truck_type_all',\n",
    "                     'vector_regular_cargo_truck_length_all', 'vector_regular_cargo_line_30',\n",
    "                     'vector_regular_cargo_truck_type_30', 'vector_regular_cargo_truck_length_30']\n",
    "\n",
    "    sparse_feature = data.drop(columns=['label'] + dense_feature).columns.tolist()\n",
    "    # print(len(sparse_feature))\n",
    "    print('数据维度:', data.shape, '标签比例:', data['label'].value_counts())\n",
    "    data[sparse_feature] = data[sparse_feature].astype('uint8')\n",
    "    target = ['label']\n",
    "\n",
    "    feat_sizes = {}  # 初始化一个空字典 feat_sizes。\n",
    "    feat_sizes_dense = {feat: 1 for feat in dense_feature}\n",
    "    # 对每个稀疏特征创建一个键值对，键为特征名称，值为该特征在数据中唯一取值的数量（即不同的类别个数）。\n",
    "    feat_sizes_sparse = {feat: len(data[feat].unique()) for feat in sparse_feature}\n",
    "    # 将稠密特征和稀疏特征的维度大小更新到 feat_sizes 字典中，得到包含所有特征维度大小信息的字典 feat_sizes。\n",
    "    feat_sizes.update(feat_sizes_dense)\n",
    "    feat_sizes.update(feat_sizes_sparse)\n",
    "\n",
    "    # 定义fixlen_feature_columns，包含了所有特征的名称和类型。\n",
    "    fixlen_feature_columns = [(feat, 'sparse') for feat in sparse_feature] + [(feat, 'dense') for feat in dense_feature]\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    # 数据集划分\n",
    "    train_val, test = train_test_split(data, test_size=0.1, random_state=seed)\n",
    "    train, validation = train_test_split(train_val, test_size=0.1, random_state=seed)\n",
    "\n",
    "    # 每个数据集中 0 和 1 标签的数量\n",
    "    train_label_summary = train['label'].value_counts()\n",
    "    validation_label_summary = validation['label'].value_counts()\n",
    "    test_label_summary = test['label'].value_counts()\n",
    "\n",
    "    # 输出结果\n",
    "    print(\"训练集标签汇总:\\n\", train_label_summary)\n",
    "    print(\"验证集标签汇总:\\n\", validation_label_summary)\n",
    "    print(\"测试集标签汇总:\\n\", test_label_summary)\n",
    "\n",
    "\n",
    "    # DataLoader准备\n",
    "    def create_data_loader(df, batch_size):\n",
    "        labels = pd.DataFrame(df['label'])\n",
    "        features = df.drop(columns=['label'])\n",
    "        tensor_data = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(labels)))\n",
    "        return DataLoader(tensor_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    train_loader = create_data_loader(train, batch_size)\n",
    "    validation_loader = create_data_loader(validation, batch_size)\n",
    "    test_loader = create_data_loader(test, batch_size)\n",
    "\n",
    "    # 模型初始化\n",
    "    #device = 'cuda:0'\n",
    "    device='cpu'\n",
    "    model = MSB_CMNet(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns).to(device)\n",
    "    loss_func = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        total_loss_epoch = 0.0\n",
    "        total_tmp = 0\n",
    "        model.train()\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_pred = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            total_tmp += 1\n",
    "\n",
    "        # 验证集评估\n",
    "        validation_loss, validation_auc = get_metrics(validation_loader, model)\n",
    "        print(\n",
    "            f'Epoch {epoch}/{epoches}, Train_loss: {total_loss_epoch / total_tmp:.4f}, Validation_loss: {validation_loss:.4f},Validation AUC: {validation_auc:.4f}')\n",
    "\n",
    "    # 测试集评估\n",
    "    test_loss, final_test_auc = get_metrics(test_loader, model)\n",
    "    print(f'Test_loss:{test_loss:.4f},Final Test AUC: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cf8ec-5d27-43c7-911a-a8ca713a35da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
