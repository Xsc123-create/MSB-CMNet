{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c082cf7-c9ab-48d1-a493-43bdc9f32275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Epoch 0/40, Train Loss: 0.2527, Validation_loss: 0.2301,Validation AUC: 0.6836\n",
      "Epoch 1/40, Train Loss: 0.2260, Validation_loss: 0.2235,Validation AUC: 0.7230\n",
      "Epoch 2/40, Train Loss: 0.2222, Validation_loss: 0.2208,Validation AUC: 0.7355\n",
      "Epoch 3/40, Train Loss: 0.2198, Validation_loss: 0.2181,Validation AUC: 0.7481\n",
      "Epoch 4/40, Train Loss: 0.2175, Validation_loss: 0.2176,Validation AUC: 0.7511\n",
      "Epoch 5/40, Train Loss: 0.2158, Validation_loss: 0.2166,Validation AUC: 0.7563\n",
      "Epoch 6/40, Train Loss: 0.2145, Validation_loss: 0.2137,Validation AUC: 0.7618\n",
      "Epoch 7/40, Train Loss: 0.2129, Validation_loss: 0.2145,Validation AUC: 0.7637\n",
      "Epoch 8/40, Train Loss: 0.2116, Validation_loss: 0.2118,Validation AUC: 0.7672\n",
      "Epoch 9/40, Train Loss: 0.2103, Validation_loss: 0.2113,Validation AUC: 0.7698\n",
      "Epoch 10/40, Train Loss: 0.2093, Validation_loss: 0.2098,Validation AUC: 0.7733\n",
      "Epoch 11/40, Train Loss: 0.2084, Validation_loss: 0.2094,Validation AUC: 0.7764\n",
      "Epoch 12/40, Train Loss: 0.2071, Validation_loss: 0.2090,Validation AUC: 0.7756\n",
      "Epoch 13/40, Train Loss: 0.2059, Validation_loss: 0.2080,Validation AUC: 0.7798\n",
      "Epoch 14/40, Train Loss: 0.2050, Validation_loss: 0.2076,Validation AUC: 0.7813\n",
      "Epoch 15/40, Train Loss: 0.2043, Validation_loss: 0.2077,Validation AUC: 0.7814\n",
      "Epoch 16/40, Train Loss: 0.2033, Validation_loss: 0.2064,Validation AUC: 0.7835\n",
      "Epoch 17/40, Train Loss: 0.2024, Validation_loss: 0.2064,Validation AUC: 0.7844\n",
      "Epoch 18/40, Train Loss: 0.2015, Validation_loss: 0.2054,Validation AUC: 0.7873\n",
      "Epoch 19/40, Train Loss: 0.2005, Validation_loss: 0.2059,Validation AUC: 0.7857\n",
      "Epoch 20/40, Train Loss: 0.1998, Validation_loss: 0.2055,Validation AUC: 0.7881\n",
      "Epoch 21/40, Train Loss: 0.1989, Validation_loss: 0.2045,Validation AUC: 0.7898\n",
      "Epoch 22/40, Train Loss: 0.1979, Validation_loss: 0.2054,Validation AUC: 0.7885\n",
      "Epoch 23/40, Train Loss: 0.1973, Validation_loss: 0.2034,Validation AUC: 0.7923\n",
      "Epoch 24/40, Train Loss: 0.1966, Validation_loss: 0.2025,Validation AUC: 0.7945\n",
      "Epoch 25/40, Train Loss: 0.1955, Validation_loss: 0.2021,Validation AUC: 0.7949\n",
      "Epoch 26/40, Train Loss: 0.1950, Validation_loss: 0.2024,Validation AUC: 0.7954\n",
      "Epoch 27/40, Train Loss: 0.1943, Validation_loss: 0.2016,Validation AUC: 0.7967\n",
      "Epoch 28/40, Train Loss: 0.1936, Validation_loss: 0.2023,Validation AUC: 0.7965\n",
      "Epoch 29/40, Train Loss: 0.1931, Validation_loss: 0.2022,Validation AUC: 0.7972\n",
      "Epoch 30/40, Train Loss: 0.1925, Validation_loss: 0.2011,Validation AUC: 0.7982\n",
      "Epoch 31/40, Train Loss: 0.1921, Validation_loss: 0.2018,Validation AUC: 0.7996\n",
      "Epoch 32/40, Train Loss: 0.1915, Validation_loss: 0.2028,Validation AUC: 0.7997\n",
      "Epoch 33/40, Train Loss: 0.1903, Validation_loss: 0.2009,Validation AUC: 0.8020\n",
      "Early stopping triggered at epoch 33, Validation AUC: 0.8020\n",
      "Test_loss:0.1988,Final Test AUC: 0.8043\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "\n",
    "\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate, ):\n",
    "        super(DNN, self).__init__()\n",
    "        self.inputs_dim = inputs_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.hidden_units = [inputs_dim] + list(self.hidden_units)\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.0001)\n",
    "\n",
    "        # self.bn = nn.ModuleList([\n",
    "        #     nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        # ])\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        inputs = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](inputs)\n",
    "            fc = self.activation(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            inputs = fc\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class InteractingLayer(nn.Module):\n",
    "    def __init__(self, embedding_size, head_num=2, use_res=True, scaling=False):\n",
    "        super(InteractingLayer, self).__init__()\n",
    "        self.att_embedding_size = embedding_size // head_num\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.scaling = scaling\n",
    "\n",
    "        self.W_Query = nn.Parameter(torch.Tensor(embedding_size, embedding_size))\n",
    "        self.W_Key = nn.Parameter(torch.Tensor(embedding_size, embedding_size))\n",
    "        self.W_Value = nn.Parameter(torch.Tensor(embedding_size, embedding_size))\n",
    "\n",
    "        if self.use_res:\n",
    "            self.W_Res = nn.Parameter(torch.Tensor(embedding_size, embedding_size))\n",
    "        for tensor in self.parameters():\n",
    "            nn.init.normal_(tensor, mean=0.0, std=0.05)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # inputs: [1024, 26, 4]\n",
    "        #keys: [1024, 26, 4]\n",
    "        querys = torch.tensordot(inputs, self.W_Query, dims=([-1], [0]))\n",
    "        keys = torch.tensordot(inputs, self.W_Key, dims=([-1], [0]))\n",
    "        values = torch.tensordot(inputs, self.W_Value, dims=([-1], [0]))\n",
    "\n",
    "        # keys: [2, 1024, 26, 2]\n",
    "        querys = torch.stack(torch.split(querys, self.att_embedding_size, dim=2))\n",
    "        keys = torch.stack(torch.split(keys, self.att_embedding_size, dim=2))\n",
    "        values = torch.stack(torch.split(values, self.att_embedding_size, dim=2))\n",
    "\n",
    "        # inner_product: [2, 1024, 26, 26]\n",
    "        inner_product = torch.einsum('bnik,bnjk->bnij', querys, keys)\n",
    "\n",
    "        if self.scaling:\n",
    "            inner_product /= self.att_embedding_size ** 0.5\n",
    "        self.normalized_att_scores = F.softmax(inner_product, dim=-1)\n",
    "\n",
    "        # [2, 1024, 26, 2]\n",
    "        result = torch.matmul(self.normalized_att_scores, values)\n",
    "        # [1, 1024, 26, 4]\n",
    "        result = torch.cat(torch.split(result, 1, ), dim=-1)\n",
    "        # [1024, 26, 4]\n",
    "        result = torch.squeeze(result, dim=0)\n",
    "        if self.use_res:\n",
    "            result += torch.tensordot(inputs, self.W_Res, dims=([-1], [0]))\n",
    "        result = F.relu(result)\n",
    "        return result\n",
    "\n",
    "class AutoInt(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns, att_layer_num=3,\n",
    "                 att_head_num=2,\n",
    "                 att_res=True, dnn_hidden_units=(256, 128)):\n",
    "        super(AutoInt, self).__init__()\n",
    "        self.sparse_feature_columns = list(filter(lambda x: x[1] == 'sparse', dnn_feature_columns))\n",
    "        self.embedding_dic = nn.ModuleDict({\n",
    "            feat[0]: nn.Embedding(feat_size[feat[0]], embedding_size, sparse=False) for feat in\n",
    "            self.sparse_feature_columns\n",
    "        })\n",
    "        self.dense_feature_columns = list(filter(lambda x: x[1] == 'dense', dnn_feature_columns))\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        start = 0\n",
    "        for feat in feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "        self.dnn = DNN(len(self.dense_feature_columns) + embedding_size * len(self.embedding_dic), dnn_hidden_units,\n",
    "                       0.5)\n",
    "\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1] + embedding_size * len(self.embedding_dic), 1, bias=False)\n",
    "\n",
    "        for name, tensor in self.dnn_linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.00001)\n",
    "\n",
    "        dnn_hidden_units = [len(feat_size), 1]\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(dnn_hidden_units[i], dnn_hidden_units[i + 1]) for i in range(len(dnn_hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.00001)\n",
    "\n",
    "        self.int_layers = nn.ModuleList([\n",
    "            InteractingLayer(embedding_size, att_head_num, att_res) for _ in range(att_layer_num)\n",
    "        ])\n",
    "\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # # X [1024, 39]\n",
    "        # logit = X\n",
    "        # for i in range(len(self.linear)):\n",
    "        #     fc = self.linear[i](logit)\n",
    "        #     fc = self.act(fc)\n",
    "        #     fc = self.dropout(fc)\n",
    "        #     logit = fc\n",
    "        # logit [1024, 1]\n",
    "        sparse_embedding = [\n",
    "            self.embedding_dic[feat[0]](X[:, self.feature_index[feat[0]]].long()).reshape(X.shape[0], 1, -1)\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)\n",
    "        sparse_input = torch.flatten(sparse_input, start_dim=1)\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "\n",
    "        # att_input [1024, 26, 4]\n",
    "        att_input = torch.cat(sparse_embedding, dim=1)\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "        # att_out [1024, 104]\n",
    "        att_output = torch.flatten(att_input, start_dim=1)\n",
    "        #print('att_output shape', att_output.shape)\n",
    "\n",
    "        # dnn_input [1024, 117] 26*4+13\n",
    "        dnn_input = torch.cat((dense_input, sparse_input), dim=1)\n",
    "        # deep_out [1024, 128]\n",
    "        deep_out = self.dnn(dnn_input)\n",
    "        stack_out = torch.cat((att_output, deep_out), dim=-1)\n",
    "        #print('stack_out shape', stack_out.shape)\n",
    "        # logit += self.dnn_linear(stack_out)\n",
    "        final= self.dnn_linear(stack_out)\n",
    "        #print('logit shape', logit.shape)\n",
    "        y_pred = torch.sigmoid(final)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size =1024\n",
    "    lr = 1e-3\n",
    "    wd = 1e-5\n",
    "    epoches = 40\n",
    "    seed = 2022\n",
    "    embedding_size =10\n",
    "    device = 'cuda:0'\n",
    "    # pd.set_option('display.max_rows', None)  # 显示数据中所有的列\n",
    "    data = pd.read_csv('vehicle_data_model_50w_3.csv')\n",
    "    # data = pd.read_csv('vehicle_data_model_30w_2.csv')\n",
    "    dense_feature= ['driver_auth_success_days','cargo_search_cnt_3','cargo_search_cnt_7','scan_cargo_cnt_3','scan_cargo_cnt_7','click_cargo_cnt_3_x','click_cargo_cnt_7','call_cnt_3_driver','call_cnt_7_driver',\n",
    "               'shipper_auth_success_days','exposure_cargo_cnt_3','exposure_cnt_3','click_cargo_cnt_3_y','click_cnt_3','cargo_weight','vector_regular_subscribe_line',\n",
    "                       'vector_regular_cargo_line_all','vector_regular_cargo_truck_type_all','vector_regular_cargo_truck_length_all','vector_regular_cargo_line_30',\n",
    "                       'vector_regular_cargo_truck_type_30','vector_regular_cargo_truck_length_30']\n",
    "\n",
    "\n",
    "    # 假设你的数据集中包含'label'列，并且dense_feature已经定义\n",
    "    sparse_feature  = data.drop(columns=['label'] + dense_feature).columns.tolist()\n",
    "    print(len(sparse_feature))\n",
    "    pd.options.display.max_rows = None  # 显示所有列\n",
    "    data[sparse_feature]=data[sparse_feature].astype('uint8')\n",
    "    target = ['label']\n",
    "\n",
    "    feat_sizes = {}  # 初始化一个空字典 feat_sizes。\n",
    "    feat_sizes_dense = {feat: 1 for feat in dense_feature}#这里将稠密特征的维度大小设置为1，因为这些特征不需要经过 Embedding 层，直接作为输入。\n",
    "    # 对每个稀疏特征创建一个键值对，键为特征名称，值为该特征在数据中唯一取值的数量（即不同的类别个数）。\n",
    "    feat_sizes_sparse = {feat: len(data[feat].unique()) for feat in sparse_feature}\n",
    "    # 将稠密特征和稀疏特征的维度大小更新到 feat_sizes 字典中，得到包含所有特征维度大小信息的字典 feat_sizes。\n",
    "    feat_sizes.update(feat_sizes_dense)\n",
    "    feat_sizes.update(feat_sizes_sparse)\n",
    "    # for feat in sparse_feature:\n",
    "    #     lbe = LabelEncoder()  #使用LabelEncoder类对每个稀疏特征进行编码,将每个特征的字符串类型的值映射成整数编号\n",
    "    #     data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "\n",
    "    # 定义fixlen_feature_columns，包含了所有特征的名称和类型（sparse或dense）。\n",
    "    fixlen_feature_columns = [(feat, 'sparse') for feat in sparse_feature] + [(feat, 'dense') for feat in dense_feature]\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "   \n",
    "    # 数据集划分\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # DataLoader准备\n",
    "    def create_data_loader(df, batch_size):\n",
    "        labels = pd.DataFrame(df['label'])\n",
    "        features = df.drop(columns=['label'])\n",
    "        tensor_data = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(labels)))\n",
    "        return DataLoader(tensor_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    train_loader = create_data_loader(train, batch_size)\n",
    "    validation_loader = create_data_loader(validation, batch_size)\n",
    "    test_loader = create_data_loader(test, batch_size)\n",
    "\n",
    "    # 模型初始化\n",
    "    # model = FiBiNET(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns)\n",
    "    device = 'cuda:0'\n",
    "    model = AutoInt(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns).to(device)\n",
    "    loss_func = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 早停策略参数\n",
    "    early_stopping_threshold = 0.80\n",
    "    best_validation_auc = 0\n",
    "    \n",
    "    # 定义一个函数来计算给定数据加载器上的平均损失和AUC\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device).float(), y.to(device).float()\n",
    "                y_hat = model(x)\n",
    "                loss = loss_func(y_hat, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_samples += x.size(0)\n",
    "        avg_loss = total_loss / total_samples\n",
    "        auc = get_auc(loader, model)\n",
    "        return avg_loss, auc\n",
    "\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        total_loss_epoch = 0.0\n",
    "        total_tmp = 0\n",
    "        model.train()\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            total_tmp += 1\n",
    "\n",
    "        # 验证集评估\n",
    "        validation_loss, validation_auc = evaluate(validation_loader)\n",
    "        print(\n",
    "            f'Epoch {epoch}/{epoches}, Train Loss: {total_loss_epoch / total_tmp:.4f}, Validation_loss: {validation_loss:.4f},Validation AUC: {validation_auc:.4f}')\n",
    "\n",
    "        # 更新最佳验证集AUC\n",
    "        if validation_auc > best_validation_auc:\n",
    "            best_validation_auc = validation_auc\n",
    "\n",
    "        # 早停判断\n",
    "        if validation_auc >= early_stopping_threshold:\n",
    "            print(f'Early stopping triggered at epoch {epoch}, Validation AUC: {validation_auc:.4f}')\n",
    "            break\n",
    "\n",
    "    # 测试集评估\n",
    "    test_loss, final_test_auc = evaluate(test_loader)\n",
    "    print(f'Test_loss:{test_loss:.4f},Final Test AUC: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32cd70-f027-4e2a-aa58-733b69e80949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
