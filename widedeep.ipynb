{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a154822e-ce32-47ff-9f86-1da99979cf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Epoch 0/50, Train Loss: 0.2570, Validation_loss: 0.2355,Validation AUC: 0.6310\n",
      "Epoch 1/50, Train Loss: 0.2359, Validation_loss: 0.2263,Validation AUC: 0.7023\n",
      "Epoch 2/50, Train Loss: 0.2286, Validation_loss: 0.2224,Validation AUC: 0.7347\n",
      "Epoch 3/50, Train Loss: 0.2252, Validation_loss: 0.2217,Validation AUC: 0.7425\n",
      "Epoch 4/50, Train Loss: 0.2231, Validation_loss: 0.2194,Validation AUC: 0.7447\n",
      "Epoch 5/50, Train Loss: 0.2217, Validation_loss: 0.2164,Validation AUC: 0.7511\n",
      "Epoch 6/50, Train Loss: 0.2200, Validation_loss: 0.2152,Validation AUC: 0.7566\n",
      "Epoch 7/50, Train Loss: 0.2186, Validation_loss: 0.2144,Validation AUC: 0.7587\n",
      "Epoch 8/50, Train Loss: 0.2169, Validation_loss: 0.2133,Validation AUC: 0.7631\n",
      "Epoch 9/50, Train Loss: 0.2162, Validation_loss: 0.2132,Validation AUC: 0.7648\n",
      "Epoch 10/50, Train Loss: 0.2150, Validation_loss: 0.2126,Validation AUC: 0.7672\n",
      "Epoch 11/50, Train Loss: 0.2142, Validation_loss: 0.2117,Validation AUC: 0.7687\n",
      "Epoch 12/50, Train Loss: 0.2129, Validation_loss: 0.2117,Validation AUC: 0.7712\n",
      "Epoch 13/50, Train Loss: 0.2118, Validation_loss: 0.2100,Validation AUC: 0.7756\n",
      "Epoch 14/50, Train Loss: 0.2111, Validation_loss: 0.2098,Validation AUC: 0.7756\n",
      "Epoch 15/50, Train Loss: 0.2101, Validation_loss: 0.2092,Validation AUC: 0.7767\n",
      "Epoch 16/50, Train Loss: 0.2088, Validation_loss: 0.2098,Validation AUC: 0.7795\n",
      "Epoch 17/50, Train Loss: 0.2078, Validation_loss: 0.2089,Validation AUC: 0.7788\n",
      "Epoch 18/50, Train Loss: 0.2069, Validation_loss: 0.2072,Validation AUC: 0.7814\n",
      "Epoch 19/50, Train Loss: 0.2057, Validation_loss: 0.2070,Validation AUC: 0.7835\n",
      "Epoch 20/50, Train Loss: 0.2053, Validation_loss: 0.2066,Validation AUC: 0.7860\n",
      "Epoch 21/50, Train Loss: 0.2045, Validation_loss: 0.2057,Validation AUC: 0.7865\n",
      "Epoch 22/50, Train Loss: 0.2036, Validation_loss: 0.2064,Validation AUC: 0.7876\n",
      "Epoch 23/50, Train Loss: 0.2026, Validation_loss: 0.2059,Validation AUC: 0.7879\n",
      "Epoch 24/50, Train Loss: 0.2016, Validation_loss: 0.2045,Validation AUC: 0.7903\n",
      "Epoch 25/50, Train Loss: 0.2009, Validation_loss: 0.2047,Validation AUC: 0.7911\n",
      "Epoch 26/50, Train Loss: 0.2004, Validation_loss: 0.2036,Validation AUC: 0.7911\n",
      "Epoch 27/50, Train Loss: 0.1994, Validation_loss: 0.2052,Validation AUC: 0.7919\n",
      "Epoch 28/50, Train Loss: 0.1990, Validation_loss: 0.2036,Validation AUC: 0.7941\n",
      "Epoch 29/50, Train Loss: 0.1985, Validation_loss: 0.2039,Validation AUC: 0.7925\n",
      "Epoch 30/50, Train Loss: 0.1973, Validation_loss: 0.2033,Validation AUC: 0.7938\n",
      "Epoch 31/50, Train Loss: 0.1973, Validation_loss: 0.2019,Validation AUC: 0.7956\n",
      "Epoch 32/50, Train Loss: 0.1960, Validation_loss: 0.2018,Validation AUC: 0.7957\n",
      "Epoch 33/50, Train Loss: 0.1957, Validation_loss: 0.2044,Validation AUC: 0.7935\n",
      "Epoch 34/50, Train Loss: 0.1952, Validation_loss: 0.2023,Validation AUC: 0.7949\n",
      "Epoch 35/50, Train Loss: 0.1945, Validation_loss: 0.2018,Validation AUC: 0.7960\n",
      "Epoch 36/50, Train Loss: 0.1936, Validation_loss: 0.2015,Validation AUC: 0.7975\n",
      "Epoch 37/50, Train Loss: 0.1937, Validation_loss: 0.2020,Validation AUC: 0.7979\n",
      "Epoch 38/50, Train Loss: 0.1930, Validation_loss: 0.2020,Validation AUC: 0.7979\n",
      "Epoch 39/50, Train Loss: 0.1927, Validation_loss: 0.2018,Validation AUC: 0.7980\n",
      "Epoch 40/50, Train Loss: 0.1921, Validation_loss: 0.2020,Validation AUC: 0.7970\n",
      "Epoch 41/50, Train Loss: 0.1919, Validation_loss: 0.2031,Validation AUC: 0.7965\n",
      "Epoch 42/50, Train Loss: 0.1909, Validation_loss: 0.2020,Validation AUC: 0.7979\n",
      "Epoch 43/50, Train Loss: 0.1907, Validation_loss: 0.2015,Validation AUC: 0.7984\n",
      "Epoch 44/50, Train Loss: 0.1905, Validation_loss: 0.2010,Validation AUC: 0.7997\n",
      "Epoch 45/50, Train Loss: 0.1896, Validation_loss: 0.2014,Validation AUC: 0.7986\n",
      "Epoch 46/50, Train Loss: 0.1892, Validation_loss: 0.2006,Validation AUC: 0.8003\n",
      "Early stopping triggered at epoch 46, Validation AUC: 0.8003\n",
      "Test_loss:0.1973,Final Test AUC: 0.8078\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "\n",
    "\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate, ):\n",
    "        super(DNN, self).__init__()\n",
    "        self.inputs_dim = inputs_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.hidden_units = [inputs_dim] + list(self.hidden_units)\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.0001)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        inputs = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](inputs)\n",
    "            fc = self.activation(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            inputs = fc\n",
    "        return inputs\n",
    "\n",
    "class Wide_Deep(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns,\n",
    "                 use_attention=True, attention_factor=8, l2_reg=0.00001, drop_rate=0.9, dnn_hidden_units=(256, 128)):\n",
    "        super(Wide_Deep, self).__init__()\n",
    "        self.sparse_feature_columns = list(filter(lambda x: x[1] == 'sparse', dnn_feature_columns))\n",
    "        self.embedding_dic = nn.ModuleDict({\n",
    "            feat[0]: nn.Embedding(feat_size[feat[0]], embedding_size, sparse=False) for feat in\n",
    "            self.sparse_feature_columns\n",
    "        })\n",
    "        self.dense_feature_columns = list(filter(lambda x: x[1] == 'dense', dnn_feature_columns))\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        start = 0\n",
    "        for feat in feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "        self.dnn = DNN(len(self.dense_feature_columns) + embedding_size * len(self.embedding_dic), dnn_hidden_units,\n",
    "                       0.5)\n",
    "\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1] , 1, bias=False)\n",
    "\n",
    "        dnn_hidden_units = [len(feat_size), 1]\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(dnn_hidden_units[i], dnn_hidden_units[i + 1]) for i in range(len(dnn_hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.00001)\n",
    "\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # wide\n",
    "        logit = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](logit)\n",
    "            fc = self.act(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            logit = fc\n",
    "\n",
    "        # deep\n",
    "        sparse_embedding = [\n",
    "            self.embedding_dic[feat[0]](X[:, self.feature_index[feat[0]]].long()).reshape(X.shape[0], 1, -1)\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)\n",
    "        sparse_input = torch.flatten(sparse_input, start_dim=1)\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "        dnn_input = torch.cat((sparse_input, dense_input), dim=1)\n",
    "        dnn_out = self.dnn(dnn_input)\n",
    "        dnn_logit = self.dnn_linear(dnn_out)\n",
    "        logit += dnn_logit\n",
    "\n",
    "        y_pred = torch.sigmoid(logit)\n",
    "        return y_pred\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "\n",
    "\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate, ):\n",
    "        super(DNN, self).__init__()\n",
    "        self.inputs_dim = inputs_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.hidden_units = [inputs_dim] + list(self.hidden_units)\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.0001)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        inputs = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](inputs)\n",
    "            fc = self.activation(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            inputs = fc\n",
    "        return inputs\n",
    "\n",
    "class Wide_Deep(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns,\n",
    "                 use_attention=True, attention_factor=8, l2_reg=0.00001, drop_rate=0.5, dnn_hidden_units=(256, 128)):\n",
    "        super(Wide_Deep, self).__init__()\n",
    "        self.sparse_feature_columns = list(filter(lambda x: x[1] == 'sparse', dnn_feature_columns))\n",
    "        self.embedding_dic = nn.ModuleDict({\n",
    "            feat[0]: nn.Embedding(feat_size[feat[0]], embedding_size, sparse=False) for feat in\n",
    "            self.sparse_feature_columns\n",
    "        })\n",
    "        self.dense_feature_columns = list(filter(lambda x: x[1] == 'dense', dnn_feature_columns))\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        start = 0\n",
    "        for feat in feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "        self.dnn = DNN(len(self.dense_feature_columns) + embedding_size * len(self.embedding_dic), dnn_hidden_units,\n",
    "                       0.5)\n",
    "\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1] , 1, bias=False)\n",
    "\n",
    "        dnn_hidden_units = [len(feat_size), 1]\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(dnn_hidden_units[i], dnn_hidden_units[i + 1]) for i in range(len(dnn_hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.00001)\n",
    "\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # wide\n",
    "        logit = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](logit)\n",
    "            fc = self.act(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            logit = fc\n",
    "\n",
    "        # deep\n",
    "        sparse_embedding = [\n",
    "            self.embedding_dic[feat[0]](X[:, self.feature_index[feat[0]]].long()).reshape(X.shape[0], 1, -1)\n",
    "            for feat in self.sparse_feature_columns]\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)\n",
    "        sparse_input = torch.flatten(sparse_input, start_dim=1)\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "        dnn_input = torch.cat((sparse_input, dense_input), dim=1)\n",
    "        dnn_out = self.dnn(dnn_input)\n",
    "        dnn_logit = self.dnn_linear(dnn_out)\n",
    "        logit += dnn_logit\n",
    "\n",
    "        y_pred = torch.sigmoid(logit)\n",
    "        return y_pred\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    batch_size =1024\n",
    "    lr = 1e-3\n",
    "    wd = 1e-5\n",
    "    epoches = 50\n",
    "    seed = 2022\n",
    "    embedding_size =10\n",
    "    device = 'cuda:0'\n",
    "    # pd.set_option('display.max_rows', None)  # 显示数据中所有的列\n",
    "    data = pd.read_csv('vehicle_data_model_50w_3.csv')\n",
    "    # data = pd.read_csv('vehicle_data_model_30w_2.csv')\n",
    "    dense_feature= ['driver_auth_success_days','cargo_search_cnt_3','cargo_search_cnt_7','scan_cargo_cnt_3','scan_cargo_cnt_7','click_cargo_cnt_3_x','click_cargo_cnt_7','call_cnt_3_driver','call_cnt_7_driver',\n",
    "               'shipper_auth_success_days','exposure_cargo_cnt_3','exposure_cnt_3','click_cargo_cnt_3_y','click_cnt_3','cargo_weight','vector_regular_subscribe_line',\n",
    "                       'vector_regular_cargo_line_all','vector_regular_cargo_truck_type_all','vector_regular_cargo_truck_length_all','vector_regular_cargo_line_30',\n",
    "                       'vector_regular_cargo_truck_type_30','vector_regular_cargo_truck_length_30']\n",
    "\n",
    "\n",
    "    # 假设你的数据集中包含'label'列，并且dense_feature已经定义\n",
    "    sparse_feature  = data.drop(columns=['label'] + dense_feature).columns.tolist()\n",
    "    print(len(sparse_feature))\n",
    "    pd.options.display.max_rows = None  # 显示所有列\n",
    "    data[sparse_feature]=data[sparse_feature].astype('uint8')\n",
    "    target = ['label']\n",
    "\n",
    "    feat_sizes = {}  # 初始化一个空字典 feat_sizes。\n",
    "    feat_sizes_dense = {feat: 1 for feat in dense_feature}#这里将稠密特征的维度大小设置为1，因为这些特征不需要经过 Embedding 层，直接作为输入。\n",
    "    # 对每个稀疏特征创建一个键值对，键为特征名称，值为该特征在数据中唯一取值的数量（即不同的类别个数）。\n",
    "    feat_sizes_sparse = {feat: len(data[feat].unique()) for feat in sparse_feature}\n",
    "    # 将稠密特征和稀疏特征的维度大小更新到 feat_sizes 字典中，得到包含所有特征维度大小信息的字典 feat_sizes。\n",
    "    feat_sizes.update(feat_sizes_dense)\n",
    "    feat_sizes.update(feat_sizes_sparse)\n",
    "    # for feat in sparse_feature:\n",
    "    #     lbe = LabelEncoder()  #使用LabelEncoder类对每个稀疏特征进行编码,将每个特征的字符串类型的值映射成整数编号\n",
    "    #     data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "\n",
    "    # 定义fixlen_feature_columns，包含了所有特征的名称和类型（sparse或dense）。\n",
    "    fixlen_feature_columns = [(feat, 'sparse') for feat in sparse_feature] + [(feat, 'dense') for feat in dense_feature]\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    # train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "   \n",
    "\n",
    "        # 数据集划分\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # DataLoader准备\n",
    "    def create_data_loader(df, batch_size):\n",
    "        labels = pd.DataFrame(df['label'])\n",
    "        features = df.drop(columns=['label'])\n",
    "        tensor_data = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(labels)))\n",
    "        return DataLoader(tensor_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    train_loader = create_data_loader(train, batch_size)\n",
    "    validation_loader = create_data_loader(validation, batch_size)\n",
    "    test_loader = create_data_loader(test, batch_size)\n",
    "\n",
    "    # 模型初始化\n",
    "    # model = FiBiNET(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns)\n",
    "    device = 'cuda:0'\n",
    "    model = Wide_Deep(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns).to(device)\n",
    "    loss_func = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 早停策略参数\n",
    "    early_stopping_threshold = 0.80\n",
    "    best_validation_auc = 0\n",
    "    \n",
    "    # 定义一个函数来计算给定数据加载器上的平均损失和AUC\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device).float(), y.to(device).float()\n",
    "                y_hat = model(x)\n",
    "                loss = loss_func(y_hat, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_samples += x.size(0)\n",
    "        avg_loss = total_loss / total_samples\n",
    "        auc = get_auc(loader, model)\n",
    "        return avg_loss, auc\n",
    "\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        total_loss_epoch = 0.0\n",
    "        total_tmp = 0\n",
    "        model.train()\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            total_tmp += 1\n",
    "\n",
    "        # 验证集评估\n",
    "        validation_loss, validation_auc = evaluate(validation_loader)\n",
    "        print(\n",
    "            f'Epoch {epoch}/{epoches}, Train Loss: {total_loss_epoch / total_tmp:.4f}, Validation_loss: {validation_loss:.4f},Validation AUC: {validation_auc:.4f}')\n",
    "\n",
    "        # 更新最佳验证集AUC\n",
    "        if validation_auc > best_validation_auc:\n",
    "            best_validation_auc = validation_auc\n",
    "\n",
    "        # 早停判断\n",
    "        if validation_auc >= early_stopping_threshold:\n",
    "            print(f'Early stopping triggered at epoch {epoch}, Validation AUC: {validation_auc:.4f}')\n",
    "            break\n",
    "\n",
    "    # 测试集评估\n",
    "    test_loss, final_test_auc = evaluate(test_loader)\n",
    "    print(f'Test_loss:{test_loss:.4f},Final Test AUC: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eed40a-393b-4999-80b0-0c773bfee973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
