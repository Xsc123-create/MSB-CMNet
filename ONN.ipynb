{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed0dba7-a0f5-4f4b-825c-d2bb6e8b20d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "Epoch 0/6, Train Loss: 0.2435, Validation_loss: 0.2115,Validation AUC: 0.7682\n",
      "Epoch 1/6, Train Loss: 0.2029, Validation_loss: 0.2077,Validation AUC: 0.7820\n",
      "Epoch 2/6, Train Loss: 0.1901, Validation_loss: 0.2087,Validation AUC: 0.7827\n",
      "Epoch 3/6, Train Loss: 0.1823, Validation_loss: 0.2122,Validation AUC: 0.7801\n",
      "Epoch 4/6, Train Loss: 0.1758, Validation_loss: 0.2143,Validation AUC: 0.7807\n",
      "Epoch 5/6, Train Loss: 0.1708, Validation_loss: 0.2181,Validation AUC: 0.7787\n",
      "Test_loss:0.2111,Final Test AUC: 0.7903\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "\n",
    "\n",
    "def get_auc(loader, model):\n",
    "    pred, target = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "            pred += list(y_hat.cpu().numpy())\n",
    "            target += list(y.cpu().numpy())\n",
    "    auc = roc_auc_score(target, pred)\n",
    "    return auc\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, inputs_dim, hidden_units, dropout_rate, ):\n",
    "        super(DNN, self).__init__()\n",
    "        self.inputs_dim = inputs_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.hidden_units = [inputs_dim] + list(self.hidden_units)\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_units[i], self.hidden_units[i + 1]) for i in range(len(self.hidden_units) - 1)\n",
    "        ])\n",
    "        for name, tensor in self.linear.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=0.0001)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        inputs = X\n",
    "        for i in range(len(self.linear)):\n",
    "            fc = self.linear[i](inputs)\n",
    "            fc = self.activation(fc)\n",
    "            fc = self.dropout(fc)\n",
    "            inputs = fc\n",
    "        return inputs\n",
    "\n",
    "class Interac(nn.Module):\n",
    "    def __init__(self, first_size, second_size, emb_size):\n",
    "        super(Interac, self).__init__()\n",
    "        self.emb1 = nn.Embedding(first_size, emb_size)\n",
    "        self.emb2 = nn.Embedding(second_size, emb_size)\n",
    "        nn.init.normal_(self.emb1.weight, mean=0, std=0.00001)\n",
    "\n",
    "    def forward(self, first, second):\n",
    "        frist_emb = self.emb1(first)\n",
    "        second_emb = self.emb2(second)\n",
    "        y = frist_emb * second_emb\n",
    "        return y\n",
    "\n",
    "class ONN(nn.Module):\n",
    "    def __init__(self, feat_size, embedding_size, linear_feature_columns, dnn_feature_columns,\n",
    "                 dnn_hidden_units=(256, 128), l2_reg=1e-5, dnn_dropout=0.5):\n",
    "        super(ONN, self).__init__()\n",
    "        self.sparse_feature_columns = list(filter(lambda x: x[1] == 'sparse', dnn_feature_columns))\n",
    "        self.embedding_dic = nn.ModuleDict({\n",
    "            feat[0]: nn.Embedding(feat_size[feat[0]], embedding_size, sparse=False) for feat in\n",
    "            self.sparse_feature_columns\n",
    "        })\n",
    "        self.dense_feature_columns = list(filter(lambda x: x[1] == 'dense', dnn_feature_columns))\n",
    "\n",
    "        self.feature_index = defaultdict(int)\n",
    "        start = 0\n",
    "        for feat in feat_size:\n",
    "            self.feature_index[feat] = start\n",
    "            start += 1\n",
    "\n",
    "        self.second_order_embedding_dic = self.__create_second_order_embedding_matrix(feat_size, embedding_size)\n",
    "\n",
    "        dim = self.__compute_nffm_dnn_dim(embedding_size)\n",
    "        self.dnn = DNN(int(dim), dnn_hidden_units, 0.5)\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1], 1, bias=False)\n",
    "\n",
    "        # dnn_hidden_units = [len(feat_size), 128, 1]\n",
    "        # self.linear = nn.ModuleList([\n",
    "        #     nn.Linear(dnn_hidden_units[i], dnn_hidden_units[i + 1]) for i in range(len(dnn_hidden_units) - 1)\n",
    "        # ])\n",
    "        # for name, tensor in self.linear.named_parameters():\n",
    "        #     if 'weight' in name:\n",
    "        #         nn.init.normal_(tensor, mean=0, std=0.00001)\n",
    "\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dnn_dropout)\n",
    "\n",
    "    def __create_second_order_embedding_matrix(self, feat_size, embedding_size, init_std=0.0001):\n",
    "        temp_dic = {}\n",
    "        for first_index in range(len(self.sparse_feature_columns)-1):\n",
    "            for second_index in range(first_index+1, len(self.sparse_feature_columns)):\n",
    "                first_name = self.sparse_feature_columns[first_index][0]\n",
    "                second_name = self.sparse_feature_columns[second_index][0]\n",
    "                temp_dic[first_name + '+' + second_name] = Interac(feat_size[first_name],\n",
    "                                                                    feat_size[second_name], embedding_size)\n",
    "        return nn.ModuleDict(temp_dic)\n",
    "\n",
    "    def __compute_nffm_dnn_dim(self, embedding_size):\n",
    "        x, y = len(self.sparse_feature_columns), len(self.dense_feature_columns)\n",
    "        return x*(x-1)/2*embedding_size + y\n",
    "\n",
    "    def __input_from_second_order_column(self, X):\n",
    "        second_order_embedding_list = []\n",
    "        for first_index in range(len(self.sparse_feature_columns)-1):\n",
    "            for second_index in range(first_index+1, len(self.sparse_feature_columns)):\n",
    "                first_name = self.sparse_feature_columns[first_index][0]\n",
    "                second_name = self.sparse_feature_columns[second_index][0]\n",
    "                second_order_embedding_list.append(\n",
    "                    self.second_order_embedding_dic[first_name+'+'+second_name](\n",
    "                        X[:, self.feature_index[first_name]].reshape(-1, 1).long(),\n",
    "                        X[:, self.feature_index[second_name]].reshape(-1, 1).long()\n",
    "                    )\n",
    "                )\n",
    "        return second_order_embedding_list\n",
    "\n",
    "    def forward(self, X):\n",
    "        # logit = X\n",
    "        # for i in range(len(self.linear)):\n",
    "        #     fc = self.linear[i](logit)\n",
    "        #     fc = self.act(fc)\n",
    "        #     fc = self.dropout(fc)\n",
    "        #     logit = fc\n",
    "        # logit [1024, 1]\n",
    "        dense_values = [X[:, self.feature_index[feat[0]]].reshape(-1, 1) for feat in self.dense_feature_columns]\n",
    "        dense_input = torch.cat(dense_values, dim=1)\n",
    "        sparse_embedding = self.__input_from_second_order_column(X) # list 325\n",
    "        sparse_input = torch.cat(sparse_embedding, dim=1)  # [1024, 325, 4]\n",
    "        sparse_input = torch.flatten(sparse_input, start_dim=1) # [1024, 1300]\n",
    "        dnn_input = torch.cat((dense_input, sparse_input), dim=1) # [1024, 1313]\n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "        dnn_logit = self.dnn_linear(dnn_output)\n",
    "        # final_logit = dnn_logit + logit\n",
    "\n",
    "        y_pred = torch.sigmoid(dnn_logit)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    batch_size =1024\n",
    "    lr = 1e-3\n",
    "    wd = 1e-5\n",
    "    epoches = 6\n",
    "    seed = 2022\n",
    "    embedding_size =10\n",
    "    device = 'cuda:0'\n",
    "    # pd.set_option('display.max_rows', None)  # 显示数据中所有的列\n",
    "    data = pd.read_csv('vehicle_data_model_50w_3.csv')\n",
    "    # data = pd.read_csv('vehicle_data_model_30w_2.csv')\n",
    "    dense_feature= ['driver_auth_success_days','cargo_search_cnt_3','cargo_search_cnt_7','scan_cargo_cnt_3','scan_cargo_cnt_7','click_cargo_cnt_3_x','click_cargo_cnt_7','call_cnt_3_driver','call_cnt_7_driver',\n",
    "               'shipper_auth_success_days','exposure_cargo_cnt_3','exposure_cnt_3','click_cargo_cnt_3_y','click_cnt_3','cargo_weight','vector_regular_subscribe_line',\n",
    "                       'vector_regular_cargo_line_all','vector_regular_cargo_truck_type_all','vector_regular_cargo_truck_length_all','vector_regular_cargo_line_30',\n",
    "                       'vector_regular_cargo_truck_type_30','vector_regular_cargo_truck_length_30']\n",
    "\n",
    "\n",
    "    # 假设你的数据集中包含'label'列，并且dense_feature已经定义\n",
    "    sparse_feature  = data.drop(columns=['label'] + dense_feature).columns.tolist()\n",
    "    print(len(sparse_feature))\n",
    "    pd.options.display.max_rows = None  # 显示所有列\n",
    "    data[sparse_feature]=data[sparse_feature].astype('uint8')\n",
    "    target = ['label']\n",
    "\n",
    "    feat_sizes = {}  # 初始化一个空字典 feat_sizes。\n",
    "    feat_sizes_dense = {feat: 1 for feat in dense_feature}#这里将稠密特征的维度大小设置为1，因为这些特征不需要经过 Embedding 层，直接作为输入。\n",
    "    # 对每个稀疏特征创建一个键值对，键为特征名称，值为该特征在数据中唯一取值的数量（即不同的类别个数）。\n",
    "    feat_sizes_sparse = {feat: len(data[feat].unique()) for feat in sparse_feature}\n",
    "    # 将稠密特征和稀疏特征的维度大小更新到 feat_sizes 字典中，得到包含所有特征维度大小信息的字典 feat_sizes。\n",
    "    feat_sizes.update(feat_sizes_dense)\n",
    "    feat_sizes.update(feat_sizes_sparse)\n",
    "    # for feat in sparse_feature:\n",
    "    #     lbe = LabelEncoder()  #使用LabelEncoder类对每个稀疏特征进行编码,将每个特征的字符串类型的值映射成整数编号\n",
    "    #     data[feat] = lbe.fit_transform(data[feat])\n",
    "\n",
    "\n",
    "    # 定义fixlen_feature_columns，包含了所有特征的名称和类型（sparse或dense）。\n",
    "    fixlen_feature_columns = [(feat, 'sparse') for feat in sparse_feature] + [(feat, 'dense') for feat in dense_feature]\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    # train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "\n",
    "    \n",
    "\n",
    "            # 数据集划分\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "    validation, test = train_test_split(test, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # DataLoader准备\n",
    "    def create_data_loader(df, batch_size):\n",
    "        labels = pd.DataFrame(df['label'])\n",
    "        features = df.drop(columns=['label'])\n",
    "        tensor_data = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(labels)))\n",
    "        return DataLoader(tensor_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    train_loader = create_data_loader(train, batch_size)\n",
    "    validation_loader = create_data_loader(validation, batch_size)\n",
    "    test_loader = create_data_loader(test, batch_size)\n",
    "\n",
    "    # 模型初始化\n",
    "    # model = FiBiNET(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns)\n",
    "    device = 'cuda:0'\n",
    "    model = ONN(feat_sizes, embedding_size, linear_feature_columns, dnn_feature_columns).to(device)\n",
    "    loss_func = nn.BCELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 早停策略参数\n",
    "    early_stopping_threshold = 0.79\n",
    "    best_validation_auc = 0\n",
    "    \n",
    "    # 定义一个函数来计算给定数据加载器上的平均损失和AUC\n",
    "    def evaluate(loader):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device).float(), y.to(device).float()\n",
    "                y_hat = model(x)\n",
    "                loss = loss_func(y_hat, y)\n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                total_samples += x.size(0)\n",
    "        avg_loss = total_loss / total_samples\n",
    "        auc = get_auc(loader, model)\n",
    "        return avg_loss, auc\n",
    "\n",
    "\n",
    "    # 训练循环\n",
    "    for epoch in range(epoches):\n",
    "        total_loss_epoch = 0.0\n",
    "        total_tmp = 0\n",
    "        model.train()\n",
    "        for index, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device).float(), y.to(device).float()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss_epoch += loss.item()\n",
    "            total_tmp += 1\n",
    "\n",
    "        # 验证集评估\n",
    "        validation_loss, validation_auc = evaluate(validation_loader)\n",
    "        print(\n",
    "            f'Epoch {epoch}/{epoches}, Train Loss: {total_loss_epoch / total_tmp:.4f}, Validation_loss: {validation_loss:.4f},Validation AUC: {validation_auc:.4f}')\n",
    "\n",
    "        # 更新最佳验证集AUC\n",
    "        if validation_auc > best_validation_auc:\n",
    "            best_validation_auc = validation_auc\n",
    "\n",
    "        # 早停判断\n",
    "        if validation_auc >= early_stopping_threshold:\n",
    "            print(f'Early stopping triggered at epoch {epoch}, Validation AUC: {validation_auc:.4f}')\n",
    "            break\n",
    "\n",
    "    # 测试集评估\n",
    "    test_loss, final_test_auc = evaluate(test_loader)\n",
    "    print(f'Test_loss:{test_loss:.4f},Final Test AUC: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72562630-cfbb-445c-b96d-453f40b8d0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
